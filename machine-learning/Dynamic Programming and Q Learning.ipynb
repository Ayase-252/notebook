{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## References\n",
    "[1] S. Haykin, Neural Network and Learning Machines, NJ: Prentice Hall, 2009. pp. 627-668\n",
    "\n",
    "## 1. Introduction\n",
    "Q Learning is a technique of **reinforcement learning**. Reinforcement learning get trained through interaction between an agent and its environment, where agent trys to achieve a specific goal in condition where are full of uncertainties.\n",
    "\n",
    "People are capable to make decisions in their daily live based on their own knowledge. But machines cannot if we don't translate the decision making problem which seems so natural to human into language computer can understand. \n",
    "\n",
    "## 2. Problem modeling\n",
    "What we are doing here is to **translate** those decision making problems to language that computer understands, which is apperently **a system of mathematic equations**. At first, let's consider the process to make decision. \n",
    "\n",
    "### 2. State, action and cost\n",
    "There are two objects in the stage, *agent* and *environment.\n",
    "\n",
    "Let $\\chi$ donates **state space**.\n",
    "Let $\\mathcal{A}_i$ denotes the **action space respective to state $i$**.\n",
    "Let $X_n$ denotes **states of enviroment at step n**.\n",
    "Let $A_n$ donates **action taken at step n**.\n",
    "\n",
    "We assume that the transition of environment has **Markov Property**. That says\n",
    "$p^n_{ij}(a)=p_{ij}(a)$\n",
    "where $p^n_{ij}(a) = P(X_{n+1}=j|X_n=u, A_n=a)$ A_n.\n",
    "It means that the transition probability only depends on latest state, and is not effected by those states that are more previous.\n",
    "\n",
    "Action incurs cost, and transition incurs cost too.\n",
    "We define cost function $g_n(i, a, j)$ as cost incurred in transition from $i$ to $j$ under action $a$ at step $n$.\n",
    "We define *terminal cost* as $g_K(i)$ as cost incurred if the terminal state is $i$ and is reached at step $n$.\n",
    "\n",
    "Natually, we define *cost-to-go* function as a indicator of the loss we suffered through the process.\n",
    "$$J^\\pi(i)=\\mathbb{E}[\\sum_{n=0}^\\infty{g_n(X_n, \\mu_n(X_n), X_{n+1}})|X_0=i]$$\n",
    "\n",
    "where $\\pi=\\{\\mu_1(.), \\mu_2(.), \\dots\\}$ are a set of functions $\\mu_k:\\chi\\to\\mathcal{A}$ which is the **policy at step k**. We call $\\pi$ as **policy**.\n",
    "\n",
    "Cost-to-go function can be interpreted as *cost incurred under policy $\\pi$ from initial state *X_0=i**\n",
    "\n",
    "If policy at step k $\\mu_k$ is same for all $k$, we say policy $\\pi$ is *stationary*. It means $\\pi=\\{\\mu,\\mu,\\dots\\}$.\n",
    "\n",
    "### 2. Optimal Policy\n",
    "We seeks *optimal policy* to minimize cost-to-go function $J^\\pi$. That is\n",
    "$$J^*(i)=\\underset{\\pi}{min}(J^\\pi(i))$$.\n",
    "\n",
    "We consider policy is stationary. Let $\\pi^*=\\{\\mu^*,\\mu^*,\\dots\\}$ denotes the *optimal policy*. If\n",
    "$$\\forall i\\in\\chi: J^{\\mu^*}(i)=J^*(i)$$\n",
    "\n",
    "## 3. Bellman's Optimality Criterion\n",
    "### 3. Principle of optimality\n",
    "> An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy starting from the state resuiting from the first decision.\n",
    "\n",
    "### 3. Dynamic Programming\n",
    "At frist, we consider a K-Step cost optimalization problem.\n",
    "\n",
    "The K step cost can be written as below:\n",
    "$$J_0(X_0)=\\mathbb{E}[g_K(X_K)+\\sum_{n=0}^{K-1}g_n(X_n,\\mu_n(X_n),X_{n+1})]$$\n",
    "\n",
    "Let's $\\pi^n=\\{\\pi_n,\\pi_{n-1},\\dots,\\pi_{K}\\}$ denoting the remaining policy after step $n$.\n",
    "\n",
    "The cost from step $n$ to step $K$ can be calculate as\n",
    "$$J_n(X_n)=\\mathbb{E}[g_K(X_K)+\\sum_{k=n}^{K-1}g_k(X_k,\\mu_k(X_k),X_{k+1})]$$\n",
    "\n",
    "In the light of *principle of optimality*, *optimal policy* can be figured out.\n",
    "\n",
    "First, we rewrite K step cost function as\n",
    "\n",
    "$$J_n(X_n)=\\mathbb{E}[g_n(X_n,\\mu_n(X_n),X_{n+1})+g_K(X_K)+\\sum_{k=n+1}^{K-1}g_k(X_k,\\mu_k(X_k),X_{k+1})))]$$\n",
    "\n",
    "$$J_n(X_n)=\\mathbb{E}[g_n(X_n,\\mu_n(X_n),X_{n+1})+J_{n+1}(X_{n+1})]$$\n",
    "\n",
    "If $\\pi^*$ is a optimal policy, so as $\\pi^{n*}$. So we have\n",
    "\n",
    "$$J^{\\pi^*}_n(X_n)=J^*_n(X_n)=\\mathbb{E}[g_n(X_n,\\mu_n(X_n),X_{n+1})+J^*_{n+1}(X_{n+1})]$$\n",
    "\n",
    "Optimal policy can be calculated backward. This technique is *dynamic programming*.\n",
    "\n",
    "### 3. Extend cost of $K$ steps to cost-to-go function\n",
    "It is noted that *cost-to-go* function contains a sum of cost function from index 0 to infinity. We can find the optimal policy in K-step cost optimalizaion problem. Here, we must extend cost function used in previous section to infinite step.\n",
    "\n",
    "\n",
    "From now, we consider cost function $J_n(x)$ from a different view. In previous section, we interpret it as cost incurred **from step $K-n$ to step $K$ with $X_{K-n}=x$.** Terminal state is **fixed at step K**.\n",
    "In order to control the terminal state, we redefine $J_n(x)$ as cost incurred **from step 0 to step n with $X_0=x$.**\n",
    "\n",
    "So we have n step forward cost $J_n(X_0)$.\n",
    "$$J^\\pi_n(X_0)=\\mathbb{E}[g_n(X_n)+\\sum_{k=0}^n{g_k(X_k,\\mu_k(X_k),X_{k+1})}]$$\n",
    "\n",
    "We now limit our discussion at stationary policy.\n",
    "\n",
    "As we do in dynamic programming, we write $J_n(X_0)$ as\n",
    "$$J^\\pi_n(X_0)=\\mathbb{E}[g(X_0,\\mu(X_0),X_1)+\\gamma J_{n-1}(X_1)]$$\n",
    "\n",
    "Noted that we time remaining cast with a discount factor $\\gamma$.\n",
    "Finally, we extend n to infinity.\n",
    "$$J^\\pi(X_0)=\\lim_{n\\to\\infty}J_n(X_0)$$\n",
    "\n",
    "Also, we have\n",
    "$$J^\\pi(X_0)=\\mathbb{E}[g(X_0,\\mu(X_0),X_1)+\\gamma J(X_1)]$$\n",
    "\n",
    "Before we go to the next section, we do some manipulation on above equation.\n",
    "\n",
    "First we calculate $\\mathbb{E}[g(X_0,\\mu(X_0),X_1)]$.\n",
    "\n",
    "$$\\mathbb{E}[g(X_0,\\mu(X_0),X_1)]=\\sum_{X_1}p_{X_0X_1}g(X_0,\\mu(X_0),X_1)$$\n",
    "\n",
    "where $\\sum_{X_1}$ means sum of cost function with respect to X_1 in the whole state space.\n",
    "\n",
    "In fact, $X_1$ depends on its previous state $X_0$ and the action produced by policy $\\mu(X_0)$, we use a new notation to emphasis this fact.\n",
    "\n",
    "$$c(X_0,\\mu(X_0))=\\sum_{X_1}p_{X_0X_1}g(X_0,\\mu(X_0),X_1)$$\n",
    "\n",
    "Then, second term of $J(X_0)$ can be calculated as\n",
    "\n",
    "$$\\mathbb{E}[J^*(X_1)]=\\sum_{X_1}p_{X_0,X_1}J(X_1)$$\n",
    "\n",
    "Now, we can rewrite $J(X_0)$ as\n",
    "\n",
    "$$J^\\pi(X_0)=[c(X_0,\\mu(X_0))+\\sum_{X_1}p_{X_0,X_1}J(X_1)]$$\n",
    "\n",
    "We conclude our discussion about modeling with optimalization problem depicted as\n",
    "\n",
    "Find a optimal policy $\\pi^*={\\mu^*,\\mu^*,\\dots}$ such as\n",
    "\n",
    "$$\\forall i\\in\\chi:J^{\\pi^*}(i)=J^*(i)=\\min_{\\mu}(J^\\pi(i))$$\n",
    "\n",
    "## 4. Find optimal policy\n",
    "So far, we have known what is optimal policy in terms of mathematic. Then, how to find it?, question is raised naturally. Before we go further, a concept must be introduced.\n",
    "\n",
    "### Q-factor\n",
    "Q-factor is introduced in Watkins(1989). *Q-factor for state i and action a* is defined as\n",
    "\n",
    "$$Q^\\pi(i,a)=c(i,a)+\\gamma\\sum_{j}p_{ij}(a)J^\\pi(j)$$\n",
    "\n",
    "where $a=\\mu(i)$. It means that $Q^\\mu(i,a)$ depends on $\\mu$ and $i$, and does not depends on $a$ directly.\n",
    "\n",
    "With Q-factor, cost-to-go function can be written as\n",
    "\n",
    "$$J^\\pi(X_0)=Q^\\mu(X_0,\\mu(X_0))$$\n",
    "\n",
    "### Policy Iteration Algorithm\n",
    "We are ready to find optimal policy now.\n",
    "\n",
    "首先我们先再一次确认我们面对的问题：\n",
    "> 给定所有环境关于动作$a$在状态$i$,$j$之间的转换概率$p_{ij}(a)$，求出最优策略$\\pi^*$使得\n",
    "$$\\forall i:J^{\\pi^*}=\\min_{\\mu}(J^\\pi(i))$$\n",
    "\n",
    "设$\\pi_n={\\mu_n,\\mu_n,\\dots}$是在第$n$步迭代中得到的策略。策略迭代算法首先开始于随机产生的策略$\\pi_0$，然后重复*策略评估*与*策略改良*的过程，直到策略不发生改变为止。\n",
    "1. 策略评估：在此步骤中，我们会计算出现在正在采用的策略对应的cost-to-go函数的值与所有动作以及状态对应的Q因子的值。\n",
    "2. 策略改良：在此步骤中，我们会改良现有的策略，以至于策略对所有cost-to-go函数都是*贪婪的*。\n",
    "\n",
    "#### 策略评估\n",
    "设现在算法迭代步数为$n$。状态空间$\\chi$中有$N$个状态，用$1,2,3,\\dots,N$表示。我们可以得到关于所有状态$i$的一组线性方程组。\n",
    "\n",
    "$$J^{\\pi_n}(i)=c(i,\\mu_n(i))+\\gamma\\sum^N_{j=1}p_{ij}(\\mu_n(i))J^{\\pi_n}(j) \\quad i=1,2,\\dots,N$$\n",
    "\n",
    "在上述方程组中，$J^{\\pi_n}(i) \\quad i=1,2,\\dots,N$均是未知数，方程组中有N个独立的方程，因此所有的未知数均可以通过求解方程组获得。注意到求解方程组的解等价于求解矩阵的逆，是一种非常昂贵的运算。在状态较多的时候，在实际中是不可能实现的。\n",
    "\n",
    "在获得cost-to-go函数的值之后，我们可以使用下列方程获得对于所有状态$i$,所有动作$a$的Q因子的值。\n",
    "\n",
    "$$Q^{\\pi_n}(i,a)=c(i,a)+\\gamma\\sum_{j}p_{ij}(a)J^{\\pi_n}(j)$$\n",
    "\n",
    "#### 策略改良\n",
    "一个很自然的想法就是对某一个状态，从动作空间中挑选出使其Q因子的值最小的动作作为下一步迭代的策略。即$\\pi_{n+1}={\\mu_{n+1},\\mu_{n+1},\\dots}$有：\n",
    "\n",
    "$$\\mu_{n+1}(i)=\\arg\\min_{a \\in \\mathcal{A}_i}Q^{\\pi_n}(i,a) \\quad i=1,2,\\dots,N$$\n",
    "\n",
    "注意到，我们实际上是使用上一步得到的策略得到的Q因子进行改良，所以，我们并不能保证是最优的策略。但是Q因子必然会小于或者等于上一步的结果，所以策略确实得到了改良。\n",
    "\n",
    "#### 迭代终止\n",
    "一旦策略的变化不会影响cost-to-go函数的值时，我们认为策略已经收敛到最优策略。即\n",
    "\n",
    "$$\\forall i:J^{\\pi_{n+1}}(i)=J^{\\pi_{n}}(i)$$\n",
    "\n",
    "此时迭代终止，$\\pi^*=\\pi^{n+1}$。\n",
    "\n",
    "### 值迭代\n",
    "在*策略迭代*中，我们需要在每一步中重新计算对于所有状态的cost-to-go函数的值。计算代价十分昂贵。值迭代使用*连续估计*$J^*(.)$的方法规避了这个问题。\n",
    "\n",
    "(Ross, 1983, Bertsekas, 2007)证明了值迭代算法当迭代步数趋近无穷时。对于有限时间cost-to-go函数问题，所有状态的cost-to-go函数值会收敛。\n",
    "\n",
    "令$\\hat{J}_n(i)$作为在第$n$步迭代中对状态$i$的cost-to-go函数值的估计。\n",
    "做法是，首先开始于一个随机生成的关于cost-go-function的估计$\\hat{J}_0(i)$作为初值。如果有合适的估计，也可以作为初值。\n",
    "\n",
    "在下一步迭代中，我们如此缩小对于J的估计。\n",
    "\n",
    "$$\\hat{J}_{n+1}(i)=\\min_{a \\in \\mathcal{A}}\\{ c(i,a)+\\gamma\\sum_{j=1}^Np_{ij}(a)\\hat{J}_n(j) \\}, \\quad i-1,2,\\dots,N$$\n",
    "\n",
    "当经过无限步时，$\\hat{J}_n(i)$会收敛于$J^*(i)$。\n",
    "\n",
    "此时，我们可以直接计算出对于$J^*$，所有动作和状态的Q因子，然后得出最优决策$\\pi^*$。\n",
    "\n",
    "## 估计动态规划：直接法\n",
    "上述的求解过程要求我们已知状态转移概率$p_{ij}(a)$以及状态转移的代价$g(.,.,.)$。但是，在实际问题上，我们并不是总能够得到这些值。为了得到这些值。我们可以使用Monte Carlo模拟显式地估计状态转移概率与状态转移的代价。\n",
    "\n",
    "基本思想是利用*计算机模拟*生成多条系统的轨迹。我们可以根据生成的轨迹构建状态转移的*查找表*。我们也可以记录$J(i)$,每当$i$被轨迹访问到。这样我们就可以得到状态转移代价$g(i,j)$。\n",
    "\n",
    "直接法可以与策略迭代和值迭代相结合：\n",
    "* 与策略迭代结合，我们就得到了*Q学习(Q-learning)*。\n",
    "* 与值迭代结合，我们就得到了*时间差分学习(Temporal-difference learning)*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
